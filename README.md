# DSA-Project-Group-5

---

# ğŸ§  MiniDB â€” Scalable In-Memory Data Management System

### *DS115 - Data Structures / Algorithms in Data Science*

### **Team:** DSA Group 5

---

## ğŸ¯ Objective

The goal of this project is to design and implement a **lightweight, scalable, in-memory database** capable of handling large datasets efficiently.
Our MiniDB supports:

* Search, insertion, deletion, modification, and range queries
* Graph-based relationships and traversal
* Analytical operations (min, max, average, top-K)
* Manual AVL Tree indexing for efficiency
* A minimal interactive **Streamlit** UI

This project applies the core **data structures and algorithms** learned during the semester â€” especially **AVL Trees** and **Graph Algorithms** â€” to create a functional and efficient system.

---

## ğŸ“ Folder Structure

```
MiniDB/
â”‚
â”œâ”€â”€ data/                         # Dataset files (Steam dataset or subsets)
â”‚   â”œâ”€â”€ games.csv
â”‚   â”œâ”€â”€ reviews.csv
â”‚   â””â”€â”€ developers_publishers.csv
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ storage/                  # Core storage and indexing logic
â”‚   â”‚   â”œâ”€â”€ avl_tree.py           # Manual AVL tree implementation
â”‚   â”‚   â””â”€â”€ data_store.py         # In-memory storage (list/dict)
â”‚   â”‚
â”‚   â”œâ”€â”€ query_engine/             # CRUD and range queries
â”‚   â”‚   â””â”€â”€ query_handler.py
â”‚   â”‚
â”‚   â”œâ”€â”€ graph/                    # Graph-based features
â”‚   â”‚   â”œâ”€â”€ graph_model.py
â”‚   â”‚   â””â”€â”€ graph_algorithms.py
â”‚   â”‚
â”‚   â”œâ”€â”€ analytics/                # Statistical and analytical operations
â”‚   â”‚   â””â”€â”€ analytics.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ui/                       # Streamlit frontend
â”‚   â”‚   â””â”€â”€ app.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/                    # Helpers, loaders, benchmarks
â”‚       â””â”€â”€ data_loader.py
â”‚
â”œâ”€â”€ report/
â”‚   â”œâ”€â”€ technical_report.pdf
â”‚   â””â”€â”€ performance_analysis.md
â”‚
â”œâ”€â”€ README.md                     # (You are here)
â””â”€â”€ requirements.txt
```

---

## ğŸ’» Programming Languages & Libraries

**Primary Language:** Python 3.11+

**Libraries Used:**

* `pandas` â†’ for data loading and preprocessing
* `numpy` â†’ for numerical operations
* `networkx` â†’ for graph representation and algorithms
* `streamlit` â†’ for minimal user interface
* `matplotlib` or `plotly` â†’ (optional) for visualizing analytics
* `time` â†’ for performance benchmarking

---

## ğŸ“Š Dataset

### Steam Dataset 2025 â€” Multi-Modal Gaming Analytics

ğŸ“¦ Source: [Kaggle Dataset Link](https://www.kaggle.com/datasets/crainbramp/steam-dataset-2025-multi-modal-gaming-analytics)

**Description:**

* ~239,000 Steam games and 1M+ user reviews
* Covers metadata, pricing, genres, ratings, platforms, and developerâ€“publisher networks
* 13 normalized relational tables (relational and graph-ready)
* Includes 1024-dimensional vector embeddings for semantic relationships

**Why we chose it:**

* Large-scale dataset (>1,000,000 records âœ…)
* Includes both **tabular** and **graph** structures (developer â†” publisher)
* Perfect for applying range queries, AVL indexing, and graph traversal algorithms

---

# âš™ï¸ Project Phases

---

## ğŸ”¹ **PHASE 1: Dataset Preparation**

### Step 1.1 â€” Select and Load Dataset

* Download dataset CSVs from Kaggle.
* Load into `/data` folder.
* Use `pandas` to load a manageable subset initially for testing (e.g., first 50,000 rows).

### Step 1.2 â€” Clean and Normalize Data

* Handle missing values, drop redundant columns.
* Ensure key attributes:
  `game_id`, `title`, `price`, `rating`, `genre`, `developer`, `publisher`, `release_year`.

### Step 1.3 â€” Store Raw Data

* Create a list of dictionaries:

  ```python
  [{"id": 1, "title": "Portal 2", "price": 9.99, "genre": "Puzzle", ...}, ...]
  ```
* This serves as the base layer for MiniDB.

ğŸ’¬ *Tip:* Start small for debugging. Use only a few thousand records until AVL and queries are stable.

---

## ğŸ”¹ **PHASE 2: Storage & Indexing Layer (AVL Tree)**

### Step 2.1 â€” Implement Manual AVL Tree

* Implement from scratch in `avl_tree.py`.
* Include: `insert()`, `delete()`, `search()`, and rotations (`LL`, `RR`, `LR`, `RL`).
* Keep balancing logic well-commented to demonstrate understanding.

### Step 2.2 â€” Integrate Index with Data Store

* In `data_store.py`, connect the raw dataset with AVL-based indexing.
* Example:

  ```python
  price_index = AVLTree()
  for record in data:
      price_index.insert(record["price"], record)
  ```
* This allows fast range queries and lookups by price or rating.

### Step 2.3 â€” Test AVL Tree

* Test insertion and deletion performance.
* Compare with linear search for validation.
* Print in-order traversal to verify balancing.

ğŸ’¬ *Comment:* AVL trees ensure **O(log n)** performance for search, insert, and delete â€” perfect for a large dataset.

---

## ğŸ”¹ **PHASE 3: Query Engine**

### Step 3.1 â€” CRUD Operations

Implement core MiniDB functions in `query_handler.py`:

* `search_record(key)`
* `insert_record(record)`
* `delete_record(key)`
* `update_record(key, field, value)`

### Step 3.2 â€” Range Queries

* Use AVL traversal to retrieve all records between two values.

  ```python
  results = avl.range_query(min_price=10, max_price=30)
  ```
* Return results as lists of game entries.

### Step 3.3 â€” Performance Testing

* Use Pythonâ€™s `time` library to record operation time for each query type.
* Store results in `performance_analysis.md`.

ğŸ’¬ *Tip:* Demonstrate complexity comparison (e.g., AVL vs linear search) in your final report.

---

## ğŸ”¹ **PHASE 4: Graph Features**

### Step 4.1 â€” Graph Representation

* Use `networkx` to model relationships:

  * Nodes: Developers / Publishers
  * Edges: Collaboration (if a developer and publisher worked on the same game)

### Step 4.2 â€” Implement Graph Algorithms

Add functions in `graph_algorithms.py`:

* `bfs_traversal(node)`
* `dfs_traversal(node)`
* `shortest_path(dev1, dev2)`
* `connected_components()`

### Step 4.3 â€” Visualization (Optional)

* Use `networkx.draw()` or `plotly` to show developerâ€“publisher networks.

ğŸ’¬ *Tip:* Graphs can be large â€” test traversal on subsets before scaling up.

---

## ğŸ”¹ **PHASE 5: Final Integration & UI**

### Step 5.1 â€” Build Streamlit UI

* File: `/src/ui/app.py`
* Allow users to:

  * Search by name, price, or rating
  * View range query results
  * Explore developerâ€“publisher connections
  * Display analytics (e.g., average rating by genre)

### Step 5.2 â€” Integrate Analytics

* In `analytics.py`, implement:

  * `min_price()`, `max_price()`, `avg_rating()`, `median_price()`, `top_k_games(k)`
  * Optional: `knn_similar_games(title)` using embeddings

### Step 5.3 â€” System Integration

* Connect all layers (Data â†’ AVL â†’ Query Engine â†’ Graph â†’ UI).
* Test all workflows end-to-end:

  1. Load dataset
  2. Build indexes
  3. Run search and graph queries
  4. View analytics in UI

---

## ğŸ“ˆ Performance Analysis

Include in your report:

* Time complexity for search, insertion, deletion (O(log n))
* Range query complexity (O(k + log n))
* Graph traversal complexity (O(V + E))
* Benchmarks comparing indexed vs non-indexed lookups

---

## ğŸ§© Deliverables

âœ… Fully functional MiniDB system
âœ… `README.md` (this file)
âœ… Technical Report (2â€“4 pages)
âœ… Streamlit UI demo
âœ… Presentation (final week)

---

## ğŸ“˜ References

* Kaggle Steam Dataset 2025 â€” Multi-Modal Gaming Analytics
* AVL Tree algorithms (CLRS, GeeksforGeeks, and textbook references)
* NetworkX documentation
* Streamlit official docs

---

### âœ… End of README

> *Once each phase is completed and tested, push your updates to the teamâ€™s GitHub repo. Use commits like:*
> â€œPhase 2 Complete â€” Added AVL Tree and Indexing Layer.â€

---

Would you like me to also prepare a **matching technical report template (2â€“4 pages)** with dataset schema, complexity analysis, and sample query examples next? It would pair perfectly with this README for your final submission.
